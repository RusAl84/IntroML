{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RusAl84/IntroML/blob/master/2_12_%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gPqoQG1lMG"
      },
      "source": [
        "# Рекуррентные нейронные сети.\n",
        "Каким образом вы понимаете, что урок скоро закончится и можно будет пойти отдохнуть?\n",
        "\n",
        "**ВРЕМЯ**. Вы знаете время начала и окончания урока, можете посмотреть на часы, чтобы узнать текущее время и путем несложных рассуждений посчитать, сколько времени осталось до окончания урока. Поздравляю - вы умеете предсказывать будущие события!\n",
        "\n",
        "Время упорядочивает события и играет важную роль в нашей жизни. Если мы хотим *прогнозировать* будущие события, мы должны учитывать время. А значит и модели, которые могут что-то спрогнозировать, должны время учитывать.\n",
        "\n",
        "Но все модели нейронных сетей, которые мы изучили до сих пор, со временем напрямую не работали. Их, конечно, можно заставить работать со временем. Например, добавить время как один из входов. Это возможно, но не эффективно. Ведь время может быть любое, возникает множество вопросов, как его измерять (в секундах или годах?), как использовать.\n",
        "\n",
        "Пусть мы измеряем какую-либо величину, например, температуру на улице, вчера, сегодня и завтра. Мы понимаем, что между этими измерениями есть взаимосвязь, температура на завтра зависит от температуры на сегодня (и других параметров), которая зависит от температуры на вчера. Если мы сможем установить связь между температурой сегодня и вчера, то логично предположить, что эта же связь сохранится и для температуры завтра и сейчас, а зная вид такой связи и температуру сегодня, сможем узнать температуру на завтра. Так работает *прогноз*.\n",
        "\n",
        "Что именно мы сделали?\n",
        "-  Мы представили наши измерения температуры как **последовательность** (вчера, сегодня, завтра),\n",
        "- установили некую взаимосвязь между элементами этой последовательности (сегодня и вчера),\n",
        "- предположили, а часто это оказывается правдой, что такая взаимосвязь есть между всеми, в том числе будущими, элементами последовательности (завтра и сегодня),\n",
        "- зная текущий элемент последовательности (сегодня), использую найденную взаимосвязь между элементами последовательности, посчитали будущий элемент последовательности (завтра).\n",
        "\n",
        "Если доживем до завтра, оно станет сегодня, то сможем сравнить наш прогноз и измеренную уже сегодня температуру, найти ошибку прогнозирования.\n",
        "\n",
        "Это другой подход, здесь время представлено не на прямую, мы нигде не писали сегодня это какое число, а в виде последовательности значений и мы однозначно понимаем какой элемент раньше, а какой позже.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqiDw44O0Bxs"
      },
      "source": [
        "# Нейронные сети с временными задержками\n",
        "Когда мы измеряли значения температуры в разные дни, мы **запоминали** их в последовательности. Наша модель прогноза работает по запомненным значениям. Если есть достаточная последовательность запомненных *предыдущих* значений последовательности, то мы можем обучить и какую-нибудь нейронную сеть устанавливать взаимосвязь между элементами этой последовательности, так получим модель, в которая учитывает время.\n",
        "\n",
        "Технически, мы вводим в структуру нейронной сети блоки запоминания, которые запоминают несколько значений с предыдущих шагов по времени. Использую такие блоки, можем делать любую нейронную сеть, персептрон ли, сверточную ли, которая будет понимать время.\n",
        "\n",
        "На рисунке ниже показана такая сеть, полученная из многослойного персептрона. Здесь желтым овальчиком показаны блоки запоминания (delay), которые запоминают, то, что подавалось на них на предыдущем такте времени, и возвращают это число на текущем такте времени **t**. Раз здесь блоки запоминания подключены один к другому, то получается *линия задержки*, которая возвращает несколько предыдущих значений входа: текущее x(t); задержанное на 1 такт, то есть запомненное с предыдущего такта x(t-1); задержанное на 2 такта  x(t-2) и т.п.\n",
        "(здесь время t это **номер** элемента последовательности). А уже эти запомненные с предыдущих тактов времени значения подаются на обычный многослойный персептрон, который и пытается установить взаимосвязь между элементами последовательности.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=10IpIg9cB1HM84CCRG1Dwh0_Adu1PzB3a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQtseBlu6CTW"
      },
      "source": [
        "# Рекуррентные сети\n",
        "Вообще-то, нас никто не ограничивает, какие именно сигналы внутри нейронной сети мы можем запоминать (задерживать), можно, например, задерживать выходы нейронов скрытого слоя, и подавать на следующий слой не только текущие выходы этого слоя, но и запомненные с прошлого такта по времени. Количество задержек мы выбираем сами, можно, например, запоминать не только предыдущие, но и пред-предыдущие значения.\n",
        "\n",
        "Пока мы подаем текущие и запомненные значения выходов одних слоев на слои *с большим* номером, полученные сети практически ничем не отличаются от обычных сетей. Обучаются теми же методами. Надо только учесть при обратном распространении ошибки связи с прошлыми значениями выходов. Технически это просто. Такие сети получили общее название TDNN - Time Delay Neural Networks, нейронные сети с временными задержками.\n",
        "\n",
        "Но все меняется, если мы попробуем подать запомненный сигнал (выход нейрона) на слой с меньшим или тем же номером, как на рисунке ниже (желтая линия).\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1ulRv_IIuxr3AlGTD8xwW4LkqfntRKKSI)\n",
        "\n",
        "Мы не можем сделать этого с текущим значением сигнала (выхода), иначе, чтобы посчитать его значение, нужно было бы это значения знать. А с запомненным с прошлого такта времени - можем, ведь мы его уже посчитали!\n",
        "\n",
        "Такие сети, в которых, присмотритесь к рисунку, добавленная связь идет в *обратном направлении*, чем все остальные, назвали сетями с **обратными связями**, или **рекуррентные сети** (recurrent neural networks, RNN).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtEP__hM_sAV"
      },
      "source": [
        "# Рекуррентный нейрон\n",
        "Рекуррентные сети - особенные, это другой класс нейронных сетей, с другими свойствами.\n",
        "\n",
        "Существует даже нейрон (или слой нейронов), выход которого является его же входом и никаких других входов нет.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1TDrp7wnYk1gmkA-nbI29c5gnmQ4kPxHi)\n",
        "\n",
        "Такой, казалось бы невозможный, нейрон существует, если мы вместо текущего выхода, который не знаем, будем использовать выход, запомненный с прошлого такта времени. Тогда такой нейрон возможен.\n",
        "\n",
        "**ОБРАТНАЯ СВЯЗЬ ВСЕГДА ПРЕДПОЛАГАЕТ ЗАДЕРЖКУ СИГНАЛА**, хоть часто этого не рисуют в книжках.\n",
        "\n",
        "Мы можем написать и формулу работы такого нейрона:\n",
        "\n",
        "$ y(t)=f(W*y(t-1)+b) $\n",
        "\n",
        "Чтобы получить текущий выход **y(t)**, предыдущий выход **y(t-1)** умножается на матрицу весов **W**, добавляется смещение **b** и все это преобразуется функцией активации **f** - обычная формула нейрона, не правда ли?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA-lP6d7DGcH"
      },
      "source": [
        "## Инициализация\n",
        "Первая проблема, которая нас поджидает в сетях с временными задержками, состоит в том, что задержки это память. А память нужно инициализировать, до начала расчетов заполнять ее чем-то. Это касается и рекуррентных сетей, ведь в них хотя бы одна задержка обязательно есть. Это сложная и нерешенная проблема, поэтому обычно инициализируют память (задержки) случайными числами или нулями, иногда применяют другие методы, но как лучше, никто не знает. Как вы понимаете, от инициализации решение будет зависеть очень сильно. Эта проблема схожа с инициализацией весов в градиентном спуске при обучении нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7QyDt1lD7HU"
      },
      "source": [
        "## Обучение рекуррентных сетей\n",
        "Вторая проблема рекуррентных сетей ужасна. Мы не можем обучать ее градиентным спуском!\n",
        "\n",
        "Почему? А давайте посмотрим всего лишь на один рекуррентный нейрон. Посчитав функцию ошибки, нужно посчитать производную ее по весам. Функция ошибки считается для текущего выхода y(t), но он зависит от предыдущего выхода y(t-1) и весов. Нет проблем, надо просто найти производную текущего выхода по весам. Да-да, но вот беда, предыдущий-то выход y(t-1) тоже зависит от весов W, и вы знаете как:\n",
        "\n",
        "$ y(t-1)=f(W*y(t-2)+b) $\n",
        "\n",
        "Не проблема? Применив правило дифференцирования сложных функций найдем производную и предыдущего выхода y(t-1) по весам. Но он же зависит не только от весов, но и от пред-предыдущего выхода y(t-2), который, вообще-то, зависит от весов W и выхода y(t-3)! А тот зависит от весов и y(t-4) и так до минус бесконечности.\n",
        "\n",
        "Получается, чтобы честно найти производную, нам надо бесконечно ее считать. Упс.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27tkyM1H2Zj"
      },
      "source": [
        "Что же делать? Не можем считать честно - будем считать нечестно.\n",
        "\n",
        "Нейронная сеть \"появилась\" в момент времени t=0. Разумно, что все предыдущие выходы не влияют на нее, раз их еще не существовало. Тогда в наших расчетах производной мы можем ограничиться только выходами y(t), для которых t>=0, а все остальные проигнорировать. Конечно, нужно не забыть проинициализировать задержку, величину y(-1), и ее тоже учитывать, но считать что она от весов не зависит (раз мы сами ее назначили).\n",
        "\n",
        "Разумно? Разумно. Но есть проблема, когда время t растет, нам нужно будет учитывать все больше и больше предыдущих значений: для t=10 нужны выходы с y(t-10) по y(t), а для t=1000 уже нужны выходы с y(t-1000) по текущий y(t). Многовато и неудобно.\n",
        "\n",
        "Еще сильней своей волей ограничим влияние весов на выходы. Давайте постановим, что мы учитываем влияние весов на выходы только на некоторое время назад, например, 10 тактов, а более ранние не учитываем, считая, что они не влияют. Тогда нам для любого времени t потребуется учитывать только выходы с y(t-10) по y(t).\n",
        "\n",
        "И если мы угадали, что в реальности действительно более ранние выходы не влияют, или влияют пренебрежимо мало, то модель сможет работать нормально. Но это разумное предположение, что события близкие влияют больше, чем события далекие от нас во времени, стоит ли учитывать температуру на Земле миллион лет назад, чтобы сделать прогноз на завтра?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaPexZQTLUr1"
      },
      "source": [
        "# Разворачивание рекуррентных сетей во времени\n",
        "Раз уж решили, что только некоторое число предыдущих тактов по времени влияет на текущий, то давайте их явно нарисуем и будем учитывать. Такой процесс называется разверткой (unfolding) во времени. На рисунке показана развертка на 3 такта.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1upxafDrFwb5PUW5aeAHaa2rNQ5L2cDaD)\n",
        "\n",
        "Присмотритесь внимательнее, рекуррентная сеть в развертке стала похожа на многослойную сеть, но есть отличия:\n",
        "- Каждый элемент входной последовательности **x** подается на свой слой.\n",
        "- Все слои одинаковые, имеют одни и те же веса для входов **U**, обратных связей **V** и выходов **W**.\n",
        "- Можно снимать выход **o** с каждого слоя.\n",
        "\n",
        "В оcтальном, такое представление развернутой рекуррентной сети ничем не отличается от многослойной. Значит можем обучать ее как многослойную, метод обратного распространения нам поможет. Только следует учитывать, что веса на разных слоях развертки одинаковые.\n",
        "\n",
        "Часто рекуррентные сети состоят из рекуррентных слоев нейронов, к которым добавляют обычные, не рекуррентные слои, которые обрабатывают выходы.\n",
        "\n",
        "В качестве упражнения, запишите формулу расчета выхода сети на рисунке для любого такта по времени.\n",
        "\n",
        "**Решение**\n",
        "<details>\n",
        "$ o(t)=f_o(W*h(t)+b_o) $\n",
        "\n",
        "\n",
        "$ h(t)=f_h(V*h(t-1)+b_h+U*x(t)+b_x) $\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHvO3WY7R5o8"
      },
      "source": [
        "# Примеры рекуррентных сетей\n",
        "Рекуррентные сети общего вида, полученные добавлением задержек и обратных связей в многослойные персептроны, довольно сложны. Их трудно обучать, результаты не так хороши, как нам хочется. Поэтому появилось множество модификаций, ухищрений, которые позволяют делать рекуррентные сети попроще и по эффективнее.\n",
        "\n",
        "Вообще, существуют десятки разных видов рекуррентных сетей, вплоть до хаотических, когда нейроны соединяются друг с другом случайно. В них тоже есть свои привлекательные стороны, но изучение их потребует серьезной математической подготовки.\n",
        "\n",
        "Упомянем интересную и специфичную модификацию - [сеть Хопфилда](https://ru.wikipedia.org/wiki/Нейронная_сеть_Хопфилда), но заниматься ею сегодня не будем.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сеть Элмана\n",
        "Рекуррентные слои в [torch.nn](https://pytorch.org/docs/stable/nn.html#recurrent-layers).\n",
        "\n",
        "Многослойная сеть Элмана [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN).\n",
        "\n",
        "![img](https://i.imgur.com/NIHrqIO.png)\n",
        "\n",
        "Аргументы:\n",
        "* (1) input_size  – число признаков входа x\n",
        "* (2) hidden_size – размер скрытого рекуррентного слоя h\n",
        "* (3) num_layers – число рекурретных слоев\n",
        "* nonlinearity – функция активации скрытых слоев (по умолчанию 'tanh')\n",
        "* и др.\n",
        "\n",
        "Принимает входы\n",
        "* input: входы, тензор формы  (L,N,Hin)\n",
        "* h_0: начальные значения скрытого состояния, тензор формы (D∗num_layers,N,Hout)\n",
        "\n",
        "где\n",
        "* N - размер пакета\n",
        "* L - длина последовательности\n",
        "* D=2 - для двунаправленных сетей, иначе 1\n",
        "* Hin - размер вектора входа\n",
        "* Hout - размер вектора скрытого состояния (все одинаковые, если несколько)\n",
        "* num_layers - число рекуррентных слоев\n",
        "\n",
        "Возвращает:\n",
        "\n",
        "* output: выходы, тензор формы (L,N,D∗Hout), это выход последнего рекуррентного слоя.\n",
        "\n",
        "* h_n: конечные значения скрытых слоев, тензор формы (D∗num_layers,N,Hout​)\n",
        "\n"
      ],
      "metadata": {
        "id": "zhXY26LQvc5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "rnn = nn.RNN(10, 20, 2)\n",
        "input = torch.randn(5, 3, 10)\n",
        "h0 = torch.randn(2, 3, 20)\n",
        "output, hn = rnn(input, h0)\n",
        "print(output[-1,-1,:])\n",
        "print(hn[-1,-1,:])"
      ],
      "metadata": {
        "id": "hufte1c3OzQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d51898-7c47-43c8-e646-1b5080711220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.2478,  0.3974, -0.3839, -0.3912, -0.0237,  0.0527, -0.0875,  0.0499,\n",
            "        -0.1432,  0.3986,  0.3349,  0.4760,  0.1005,  0.2626, -0.7360,  0.4591,\n",
            "         0.1013, -0.0710,  0.0431,  0.0291], grad_fn=<SliceBackward0>)\n",
            "tensor([ 0.2478,  0.3974, -0.3839, -0.3912, -0.0237,  0.0527, -0.0875,  0.0499,\n",
            "        -0.1432,  0.3986,  0.3349,  0.4760,  0.1005,  0.2626, -0.7360,  0.4591,\n",
            "         0.1013, -0.0710,  0.0431,  0.0291], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nhkGjcrmpPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}